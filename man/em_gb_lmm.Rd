% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/em_gb_lmm.R
\name{em_gb_lmm}
\alias{em_gb_lmm}
\title{EM-algorithm for Mixed Effect Gradient Boosting}
\usage{
em_gb_lmm(
  Y,
  X,
  formula_random_effects,
  data,
  gradient_params,
  initial_random_effects,
  max_iterations,
  error_tolerance,
  dom_name,
  cov_names,
  gbm_engine = "xgboost",
  ...
)
}
\arguments{
\item{Y}{numeric, target variable}

\item{X}{data.frame, Covariates}

\item{formula_random_effects}{formula, formula for Random Effects}

\item{data}{data.frame}

\item{gradient_params}{list, optional list of Gradient Boosting parameters}

\item{initial_random_effects}{numeric, initial value for domain effects}

\item{max_iterations}{integer, max number of EM-iterations}

\item{error_tolerance}{numeric, error tolerance for stop-criterion}

\item{dom_name}{factor, name of domain variable}

\item{cov_names}{name of covariates}

\item{gbm_engine}{character, choice of Gradient Boosting implementation ("xgboost", "lightgbm", "catboost")}

\item{...}{additional arguments}
}
\value{
list with results
}
\description{
This function combines Gradient Boosting with a Linear Mixed
Model (LMM) by using the Expectation-Maximization (EM) algorithm.
}
