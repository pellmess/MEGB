% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuning_megb.R
\name{tune_gbmodel}
\alias{tune_gbmodel}
\title{Tune Gradient Boosting Models with Hyperparameter Optimization}
\usage{
tune_gbmodel(
  gbm_engine,
  Y,
  X,
  smp_data,
  param_space,
  tuning_strategy = c("grid", "random", "bayesian"),
  n_iter = 20,
  seed = 42
)
}
\arguments{
\item{gbm_engine}{A string indicating the model engine: "xgboost", "lightgbm", or "catboost".}

\item{Y}{A numeric vector of the response variable.}

\item{X}{A data frame of predictor variables.}

\item{smp_data}{A data frame with the training data.}

\item{param_space}{A list of parameter ranges for tuning.}

\item{tuning_strategy}{The strategy for tuning: "grid", "random", or "bayesian".}

\item{n_iter}{integer, number of hyperparameter configurations to be evaluated when using random search or Bayesian optimization}

\item{seed}{The random seed for reproducibility.}
}
\value{
A list containing the best parameters, the best score (RMSE), and a data frame with all tuning results.
\item{best_params}{The best hyperparameters found during the tuning.}
\item{best_score}{The best evaluation score (e.g., RMSE).}
\item{results_df}{A data frame containing the evaluation results for all hyperparameter combinations.}
\item{best_iter}{The best iteration of the model.}
\item{extra}{A list with additional information, such as the Bayesian Optimization object and stop status.}
}
\description{
This function tunes a Gradient Boosting model (XGBoost, LightGBM, or CatBoost)
using grid search, random search, or Bayesian optimization.
}
